---
title: "Building in-game prediction model for final result of volleyball match"
author: "Final assignment for 'Credit Risk - methods of scorecard development in R' class under supervision of prof. Marcin Chlebus. Research conducted by Jan Dudzik"
date: "June, 2023"
output:
  html_document: default
---

## Introduction

The sports analytics field has evolved profoundly within the past decade, as a consequence of growth of applications of sophisticated computational technologies and sports data becoming more accessible. One of the primary goals of sports analytics is to predict future game outcomes by leveraging data from past performances in order to produce a competitive advantage for a team or individual. The discipline of volleyball, being a part of the Summer Olympic Games from 1964, is a sport with a substantial global audience and market share estimated to reach 332.4 million USD by the year 2028 (Business Research Insights, 2023), displaying unique potential for appliance of analytics techniques.

Volleyball teams, coaches, statisticians, and strategists could significantly benefit from predictive analytics that would enhance decision-making processes and strategic planning. Moreover, the betting industry, spectators, and the sport development sector could derive significant value from more accurate predictions of game outcomes. This research aims to fill the existing gap by building a model generating real time predictions of match outcome.

This research aims to assess several problems related to this innovative method of predicting volleyball games outcome. Firstly, it is hypothesized, that with inclusion of additional information about match progress and current squads’ performance, the accuracy of predictions increases throughout the course of the game. Secondly, the hypothesis of existence of critical point in predictions, after which the model’s performance decreases was addressed. To investigate these hypotheses, the study employs a comprehensive set of both pre-match and in-game variables to train multiple machine learning
models. The pre-match variables encapsulate the a priori historical performance of the teams, while in-game variables capture the evolving dynamics of the match state.

## Exploratory Data Analysis

```{r, include = FALSE}
# Libraries and data load -------------------------------------------------
library(readr)
library(dplyr)
library(DataExplorer)
library(lmtest)
library(caret)
library(pROC)
library(randomForest)
library(corrplot)
library(glmnet)
library(ggplot2)
library(e1071)
library(LiblineaR)
library(here)
library(gridExtra)
library(pander)


# Read the data
df <- read_csv("Plusliga_data_for_model.csv", 
               col_types = cols(Year = col_character()))
```


Originally obtained dataset consisted of 116345 datapoints from 697 matches from years 2020-2023, and contained multiple columns, that required cleaning and feature engineering. The data was webscrapped from Plusliga.com website, as part of different project. Actions for data cleaning, that were taken include:

1. Replacing null values from *Spectators* and *Relative Spectators* columns with 0
2. Converting all character columns to factors (except for MatchID)
3. Converting decimal points from columns *Sets_ratio_table_host*, *Sets_ratio_table_guest*, *Points_ratio_table_host* and *Points_ratio_table_guest* from comma to dot.
4. Grouping values from *Round* column into buckets. "play-off" and "play-out" values were grouped into "play-off" value, if *Round* was lower than 6, then "Start of the season" was picked, if those were between 7 and 19, "Mid-season" value was introduced, and in other case I assigned "End of the season", creating column *Match_time_of_season*. This operation was conducted to limit the number of factor values in this column.
5. All of the columns related to Host or Guest performance, were turned into difference between Host and Guest value. This operation was done to highlight which of the teams performed better at that time, and to reduce the number of variables and therefore limit the calculation time.

The dependent variale was *Winner*, taking value "TRUE" if the host team won the game or "FALSE" otherwise. Above mentioned operations resulted in final shape of the dataframe as presented below:

#### Pre-match variables:

* Year - The year that the match was played. Contains values between 2020 and 2022, which denote the starting year of the season.
* diff_current_position_table - The difference between the position in the season standings table of the host and guest. A positive value denotes, that the host team held better position in the standings table before the match.
* diff_matches_ratio_last_5 - The difference between host’s and guest’s balances of won/lost matches across the last 5 games. A positive value denotes, that the host team had better balance of prior 5 games compared to guest team before the match. For games happening during initial 5 rounds, the existing balance of last matches was considered.
* diff_season_points_to_matches - The difference between the ratio of season points to played matches of the host and of the guest. In Plusliga if the team that won the match 3:0 or 3:1 is awarded 3 points, while the losing team gets 0. In case of happening of tie-break, which is 5th set, the team winning 3:2 gets 2 points, while losing team receives 1. A positive value denotes, that host team scored more season points in relation to played matches, than guest before the match start.
* diff_Sets_ratio_table – The difference between the ratio of sets won to sets lost throughout the whole season for the host and for the guest. A positive value denotes, that host team had better won to lost sets ratio across the season, than guest, before the match start.
* diff_Points_ratio_table - The difference between the ratio of points won to points lost throughout the season for the host and for the guest. A positive value denotes, that host team had better won to lost points ratio across the season, than guest, before the match start.
* Time_Category - The categorical variable denoting if the game was held during a workday evening, workday late evening, weekend afternoon or weekend evening.
* Match_time_of_season - The categoric variable specifying the time of the season, based on the round number, bucketing records into four groups: Start of the season, Mid-season, End of the season and play-off.

#### In-game variables:

* Spectators - Number of people attending the game.
* Relative_spectators - Number of people attending the game divided by the stadium’s capacity.
* Set number - the number of set that the point was played in. Qualitative variable including values between 1 and 5.
* Running_net_crossings_average - the average number of times that the ball crossed the net and change of ball possession occurred. The variable is calculated as a running average in the window between start of the match and a current point.
* Current_set_difference – the difference between scored sets of the host and the guest, calculated for the time of a considered point.
* Current_point_difference - difference between scored points within the set, of the host and the guest, calculated for the time of a considered point.
* Max_point_difference_throughout_set – the largest value of the variable Current_point_difference, between the start of the match and a currently considered point.
* Max_point_difference_throughout_set – the largest value of the variable Current_point_difference, between the start of the set and a currently considered point.
* Min_point_difference_throughout_set – the lowest value of the variable Current_point_difference, between the start of the set and a currently considered point.
* Max_point_difference_throughout_match – the largest value of the variable Current_point_difference, between the start of the match and a currently considered point.
* Min_point_difference_throughout_match – the lowest value of the variable Current_point_difference, between the start of the match and a currently considered point.
* diff_Current_serve_effectiveness – Difference between serve effectiveness of host and guest, that were calculated from the beginning of match until currently considered point. Serve effectiveness is a measure used by Plusliga, identified as sum of serve aces, subtracted by number of serve errors, divided by number of all serves.
* diff_Current_positive_reception_ratio – Difference between positive reception ratio of host and guest, that were calculated from the beginning of match until currently considered point. Positive reception ratio is a measure used by Plusliga, identified as sum of serve receives classified as perfect or positive, divided by sum of all serve receives.
* diff_Current_perfect_reception_ratio - Difference between perfect reception ratio of host and guest, that were calculated from the beginning of match until currently considered point. Perfect reception ratio is a measure used by Plusliga, identified as sum of serve receives classified as perfect, divided by sum of all serve receives.
* diff_Current_negative_reception_ratio - Difference between negative reception ratio of host and guest, that were calculated from the beginning of match until currently considered point. Negative reception ratio is a measure identified as sum of serve receives classified as negative, Ball returned or receive error, divided by sum of all serve receives.
* diff_Current_attack_accuracy - Difference between attack accuracy of host and guest, that were calculated from the beginning of match until currently considered point. Attack accuracy is a measure used by Plusliga, identified as sum of hits that ended with point gained by hitter’s team, divided by all hits of this team.
* diff_Current_attack_effectiveness - Difference between attack effectiveness of host and guest, that were calculated from the beginning of match until currently considered point. Attack effectiveness is a measure used by Plusliga, identified as sum of hits that ended with point gained by hitter’s team, subtracted by all the hits that ended up with point gained by opposing team, divided by all the hits. Hits ending up with point gained by opposing team include hitting errors and hits succesfully blocked by the opposing team.
* diff_Current_timeouts – Difference between number of timeouts requested by host team and guest team, that were calculated from the beginning of match until currently considered point.
* diff_Current_challenges - Difference between number of referee’s decision challenges requested by host team and guest team, that were calculated from the beginning of match until currently considered point.

```{r, include = FALSE}
# Data transformations ----------------------------------------------------


# Switch character columns to numeric
df <- df %>%
  mutate(across(c(Sets_ratio_table_host, Sets_ratio_table_guest, Points_ratio_table_host, Points_ratio_table_guest), ~ as.numeric(gsub(",", ".", .))))

# Handling NAs
df$Spectators <- replace(df$Spectators, is.na(df$Spectators), 0)
df$Relative_spectators <- replace(df$Relative_spectators, !is.finite(df$Relative_spectators), NA)
df$Relative_spectators <- replace(df$Relative_spectators, is.na(df$Relative_spectators), mean(df$Relative_spectators, na.rm = T))

df <- df %>%
  mutate(Match_time_of_season = case_when(
    Phase %in% c("play-off", "play-out") ~ "play-off",
    !is.na(Round_original) & Round_original < 6 ~ "Start of the season",
    !is.na(Round_original) & Round_original < 20 ~ "Mid-season",
    !is.na(Round_original) ~ "End of the season",
    TRUE ~ "other"
  ))

df <- df %>%
  select(-Phase, -Round_original)

df <- df %>%
  mutate(Time_Category = factor(Time_Category),
         Year = factor(Year),
         Match_time_of_season = factor(Match_time_of_season))
# Convert selected columns to factors
df[, c("Winner", "Time_Category", "Year", "Match_time_of_season", "Set_number")] <- lapply(df[, c("Winner", "Time_Category", "Year", "Match_time_of_season", "Set_number")], factor)
# Rename levels if necessary
levels(df$Winner) <- make.names(levels(df$Winner))

# Alternatively, you can convert the response variable to a factor and allow R to automatically create valid variable names:
df$Winner <- as.factor(df$Winner)

# Replace all the null values with 0s as we will now operate on differences
df <- replace(df, is.na(df), 0)

# Calculate the differences:
host_guest_cols <- sort(colnames(df)[grep("host|guest", colnames(df))])
other_cols <- df %>% select(-all_of(host_guest_cols))
df2 <- df[, c(names(other_cols))]

# Subtract guest columns from host columns and create new columns in df2
df2$diff_Current_position_table = df$Current_position_table_host - df$Current_position_table_guest
df2$diff_Matches_ratio_last_5 = df$Matches_ratio_last_5_host - df$Matches_ratio_last_5_guest
df2$diff_Season_points_to_matches = df$Season_points_to_matches_host - df$Season_points_to_matches_guest
df2$diff_Sets_ratio_table = df$Sets_ratio_table_host - df$Sets_ratio_table_guest
df2$diff_Points_ratio_table = df$Points_ratio_table_host - df$Points_ratio_table_guest
df2$diff_Current_serve_effectiveness = df$Current_host_serve_effectiveness - df$Current_guest_serve_effectiveness
df2$diff_Current_positive_reception_ratio = df$Current_host_positive_reception_ratio - df$Current_guest_positive_reception_ratio
df2$diff_Current_perfect_reception_ratio = df$Current_host_perfect_reception_ratio - df$Current_guest_perfect_reception_ratio
df2$diff_Current_negative_reception_ratio = df$Current_host_negative_reception_ratio - df$Current_guest_negative_reception_ratio
df2$diff_Current_attack_accuracy = df$Current_host_attack_accuracy - df$Current_guest_attack_accuracy
df2$diff_Current_attack_effectiveness = df$Current_host_attack_effectiveness - df$Current_guest_attack_effectiveness
df2$diff_Current_timeouts = df$Current_timeouts_host - df$Current_timeouts_guest
df2$diff_Current_challenges = df$Current_challenges_host - df$Current_challenges_guest


# Create a data frame with each MatchID and the majority class in that match
match_summary <- df2 %>%
  group_by(MatchID) %>%
  summarise(MajorityClass = names(which.max(table(Winner))))

# Perform stratified sampling on the match_summary
set.seed(396596)
match_ids_split <- createDataPartition(match_summary$MajorityClass, p = 0.7, list = FALSE, times = 1)

# Use these splits to create your training and testing sets
train_data <- df2 %>% filter(MatchID %in% match_summary$MatchID[match_ids_split])
test_data <- df2 %>% filter(MatchID %in% match_summary$MatchID[-match_ids_split])


# Correlation matrix:
cor_matrix <- cor(train_data[,c(5,6,8:14,16:28)])
```

```{r, fig.width = 10, fig.height = 10, echo=FALSE, warning=FALSE}
library(corrplot)

corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, tl.cex = 0.7, 
         addCoef.col = "black", number.cex = 0.1, 
         diag = FALSE)


```


```{r, echo = FALSE, warning=FALSE}
df_numeric <- df2[, sapply(df2, is.numeric)]

plot_density(df_numeric)


# Function to generate bar plots for factors
plot_bar <- function(factor_var, col_name) {
  ggplot(df, aes(x = factor_var)) +
    geom_bar(fill = "grey") +
    labs(title = paste("Bar plot of", col_name)) +
    theme_bw() +
    theme(
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      axis.title.x = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.text.y = element_text(size = 12),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      axis.line = element_line(size = 0.5, color = "black")
    )
}

# Subset factor columns
factor_columns <- names(df2)[sapply(df2, is.factor)]

# Create a list to store bar plots
plots <- lapply(factor_columns, function(col) {
  col_name <- col
  plot_bar(df[[col]], col_name)
})

# Arrange plots in a 3x2 grid using grid.arrange
grid_plot <- do.call(grid.arrange, c(plots, ncol = 2))

# Convert the grid plot to markdown format
markdown_output <- paste0("```{r}\n", "print(grid_plot)\n```")



```

Above on can see the correlation plot between the variables, exposing most of the variables to be not correlated. There is definitely a positive correlation between variables denoting situation at table of standings - variables denoting performance in last 5 matches, season points, sets and point ratio, and negative correlation between those values and diff_Current_position_table - which is understandable as the bigger those values are, the smaller number denoting position in table should be. One interesting relationship is diff_Current_timeouts and Current_point_difference, pointing out, that losing teams take more timeouts, which is aligned with the theory. 

The charts depicting distributions of the numeric variables, present mostly normal or leptocurtic distributions, making those variables suitable for modelling. Some of the variables like *Current_set_difference*, *diff_Current_challenges* or *diff_Current_timeouts* could be turned into factors, as those take small number of discrete values, although I wanted to avoid creating too many dummy variables due to computation time limitations, and also those variables are ordered and provide information also as numeric variables. What is more, points difference variables have skewed distributions, and therefore could potentially be transformed into logarithmic values, but those changes didn't improve the results by much, therefore for simplicity, I decided to maintain current shape of these values.

The histograms of factor variables, present desired balance between values of dependent variable, with slightly bigger number of TRUE values, which could be interpreted by the theoretical effect of playing match as a host, which improve odds of winning. This phenomenon was addressed during train/test set division, by usage of stratified sampling method. We can also see, that "Mid-season" value of *Match_time_of_season* is quite  overpopulated. 

### Modelling

For modelling part, variety of machine learnings techniques were selected. As a benchmark for comparison, I have decided to use the Logit model mainly due to interpretability reasons. Additionally, I chose two of the most prominent tree-based models, Random Forest (Breiman, 2001), and eXtreme Gradient Boosting (Chen and Guestrin, 2016). These algorithms are recognized for their aptitude in accurately discerning intricate, latent interactions between variables. Furthermore, I chose the Support Vector Machines (Cortes and Vapnik, 1995) model, which is a more complex and computationally demanding model. The kernel function was chosen to be linear, due to limited computational resources, and cost parameter was set to 0.1 to avoid problem of overfitting, which arose in early stages of training of both considered tree-based models. In the selection, I have also incorporated the LASSO model (Tibshirani, 1996).

```{r, include = FALSE}
# Logistic Regression -----------------------------------------------------

# Modelling
formula <- Winner ~ Year + Time_Category + Spectators + Relative_spectators + diff_Current_position_table +
  diff_Matches_ratio_last_5 + diff_Season_points_to_matches + diff_Sets_ratio_table + diff_Points_ratio_table +
  Set_number + Current_point_difference + Current_set_difference + Max_point_difference_throughout_set +
  Min_point_difference_throughout_set + Max_point_difference_throughout_match + Min_point_difference_throughout_match +
  Running_net_crossings_average + diff_Current_serve_effectiveness + diff_Current_positive_reception_ratio +
  diff_Current_perfect_reception_ratio + diff_Current_negative_reception_ratio + diff_Current_attack_accuracy +
  diff_Current_attack_effectiveness + diff_Current_timeouts + diff_Current_challenges + Match_time_of_season

# 
# logit <- glm(formula,
#              # here we define type of the model
#              family =  binomial(link = "logit"),
#              data = train_data)
# logit %>% saveRDS(here("outputs", "logit.rds"))
logit <- readRDS("outputs/logit.rds")


lrtest(logit)

logit_fitted <- predict.glm(logit,
                             type = "response")
ROC.train.logit <- roc(as.numeric(train_data$Winner == "TRUE."), 
                        logit_fitted)
logit_test_fitted <- predict.glm(logit, test_data, type = "response")

ROC.test.logit <- roc(as.numeric(test_data$Winner == "TRUE."), 
                       logit_test_fitted)
table(train_data$Winner,
      ifelse(logit_fitted > 0.5, # condition
             "TRUE.", # what returned if condition TRUE
             "FALSE.")) # what returned if condition FALSE

confusionMatrix(data = as.factor(ifelse(logit_fitted > 0.5, # condition
                                        "TRUE.", # what returned if condition TRUE
                                        "FALSE.")), 
                reference = as.factor(train_data$Winner))


# Compute ECE
compute_ece <- function(predicted_probs,  n_bins = 20) {
  
  observed_labels <- ifelse(test_data$Winner == "TRUE.", 1, 0)
  
  # Create bins for the predicted probabilities
  bins <- cut(predicted_probs, breaks = seq(0, 1, length.out = n_bins + 1), include.lowest = TRUE, labels = FALSE)
  
  # Compute the average predicted probability in each bin
  bin_avgs <- tapply(predicted_probs, bins, mean)
  
  # Compute the observed frequency in each bin
  bin_true <- tapply(observed_labels, bins, mean, default = 0)
  
  # Compute the absolute difference between the predicted probability and observed frequency in each bin
  bin_diffs <- abs(bin_avgs - bin_true)
  
  # Compute the number of instances in each bin
  bin_counts <- table(bins)
  
  # Compute ECE
  ece <- sum(bin_diffs * bin_counts) / length(predicted_probs)
  
  return(ece)
}
```


```{r, include=FALSE}
# Random Forest -----------------------------------------------------------

ctrl_cv5 <- trainControl(method = "cv", 
                          number =    5,
                          classProbs = T)

# set.seed(396596)
# randomforest4 <- randomForest(formula, 
#                               data = train_data,
#                               ntree = 2000,
#                               mtry = 3,
#                               maxnodes = 300,
#                               trControl = ctrl_cv10,
#                               nodesize = 1000)


#randomforest4 %>% saveRDS(here("outputs", "randomforest4.rds"))
randomforest4 <- readRDS("outputs/randomforest4.rds")


pred.train.randomforest4 <- predict(randomforest4, 
                                    train_data, 
                                    type = "prob")[,2]

pred.test.randomforest4 <- predict(randomforest4, 
                                   test_data, 
                                   type = "prob")[,2]
```

```{r, include = FALSE}
# XGB Model ---------------------------------------------------------------


ctrl_cv5 <- trainControl(method = "repeatedcv", repeats = 5,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)


# Train the XGBoost model
parameters_xgb2 <- expand.grid(nrounds = c(500),
                               max_depth = c(2),
                               eta = c(0.1),
                               gamma = 0.2, 
                               colsample_bytree = c(0.7),
                               min_child_weight = c(1000), 
                               subsample = 0.3)


# #Train the XGBoost model
# set.seed(396596)
# xgb2 <- train(formula,
#               data = train_data,
#               method = "xgbTree",
#               trControl = ctrl_cv5,
#               tuneGrid  = parameters_xgb2)
#xgb2 %>% saveRDS(here("outputs", "xgb2.rds"))
xgb2 <- readRDS("outputs/xgb2.rds")


pred.train.xgb2 <- predict(xgb2, 
                           train_data, 
                           type = "prob")[,2]

pred.test.xgb2 <- predict(xgb2, 
                          test_data, 
                          type = "prob")[,2]
```

```{r, include = FALSE}
# SVM ---------------------------------------------------------------------

# define target and predictors
target <- train_data$Winner  # replace with your target variable name
predictors <- train_data[, -c(1,2)]
# Convert factors to dummy variables
predictors_dummy <- model.matrix(~ . - 1, data = predictors)

# Convert the predictors to a data.frame
predictors_dummy_df <- as.data.frame(predictors_dummy)

# Predict on test data
predictors_test <- test_data[, -c(1,2)]
predictors_test_dummy <- model.matrix(~ . - 1, data = predictors_test)

predictors_test_dummy <- predictors_test_dummy[, colnames(predictors_dummy)]


# set.seed(396596)
# #Train the model with probability estimates
# svm_model_prob <- svm(as.factor(target) ~ .,
#                       data = predictors_dummy_df,
#                       type = "C-classification",
#                       kernel = "linear",
#                       cost = 0.1,
#                       probability = TRUE)
# 
# svm_model_prob %>% saveRDS(here("outputs", "svm_model_prob.rds"))

svm_model_prob <- readRDS("outputs/svm_model_prob.rds")

# Convert the test predictors to a data.frame
predictors_test_dummy_df <- as.data.frame(predictors_test_dummy)

# Predict probabilities on the test data
SVM_linear_fitted_train <- predict(svm_model_prob, 
                             newdata = predictors_dummy_df, 
                             probability = TRUE)

# Predict probabilities on the test data
SVM_linear_fitted_test <- predict(svm_model_prob, 
                             newdata = predictors_test_dummy_df, 
                             probability = TRUE)

# Extract probabilities
SVM_linear_fitted_prob_train <- attr(SVM_linear_fitted_train, "probabilities")
SVM_linear_fitted_prob_test <- attr(SVM_linear_fitted_test, "probabilities")

confusionMatrix(data = as.factor(ifelse(SVM_linear_fitted_prob_test[,1] >0.5, "TRUE.", "FALSE.")), reference = test_data$Winner)
ROC.test.svm_linear  <- roc(test_data$Winner == "TRUE.", 
                            SVM_linear_fitted_prob_test[,1])
```

```{r, include = FALSE}
# LASSO -------------------------------------------------------------------


# Set up grid
parameters <- expand.grid(lambda = c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000))

set.seed(396596)
#Fit models and perform cross-validation
# lasso_model <- cv.glmnet(x = model.matrix(formula, train_data),
#                          y = train_data$Winner,
#                          family = "binomial",
#                          alpha = 1,
#                          type.measure = "auc",
#                          nfolds = 5,
#                          lambda = parameters$lambda)
# lasso_model %>% saveRDS(here("outputs", "lasso_model.rds"))
lasso_model <- readRDS("outputs/lasso_model.rds")
best_lasso <- which.max(lasso_model$cvm)

# Make predictions on train data
lasso_pred_train <- predict(lasso_model, newx = model.matrix(formula, train_data),
                      s = lasso_model$lambda[best_lasso], type = "response")

# Make predictions on test data
lasso_pred_test <- predict(lasso_model, newx = model.matrix(formula, test_data),
                      s = lasso_model$lambda[best_lasso], type = "response")

compute_ece(lasso_pred_test)
auc_lasso <- auc(test_data$Winner, lasso_pred_test)

ROC.test.Lasso  <- roc(as.numeric(test_data$Winner == "TRUE." ), 
                       lasso_pred_test)

confusionMatrix(data = as.factor(ifelse(lasso_pred_test >0.5, "TRUE.", "FALSE.")), reference = test_data$Winner)
```

### Models comparison

```{r, echo = FALSE, warning=FALSE}
accuracy <- function(preds, actuals){
  result = sum(ifelse(preds > 0.5, "TRUE.", "FALSE.") == actuals) / length(actuals)
}

comparison_df <- data.frame(
  Models = c("Logit", 
             "Random Forest", 
             "eXtreme Gradient Boosting", 
             "Support Vector Machines", 
             "Lasso"),
  ECE = c(compute_ece(logit_test_fitted),
          compute_ece(pred.test.randomforest4),
          compute_ece(pred.test.xgb2),
          compute_ece(SVM_linear_fitted_prob_test[,1]),
          compute_ece(lasso_pred_test)),
  Accuracy_test = c(accuracy(logit_test_fitted, test_data$Winner),
                    accuracy(pred.test.randomforest4, test_data$Winner),
                    accuracy(pred.test.xgb2, test_data$Winner),
                    accuracy(SVM_linear_fitted_prob_test[,1], test_data$Winner),
                    accuracy(lasso_pred_test, test_data$Winner)),
  difference = c(accuracy(logit_fitted, train_data$Winner)-accuracy(logit_test_fitted, test_data$Winner),
                 accuracy(pred.train.randomforest4, train_data$Winner)-accuracy(pred.test.randomforest4, test_data$Winner),
                 accuracy(pred.train.xgb2, train_data$Winner)-accuracy(pred.test.xgb2, test_data$Winner),
                 accuracy(SVM_linear_fitted_prob_train[,1], train_data$Winner)-accuracy(SVM_linear_fitted_prob_test[,1], test_data$Winner),
                 accuracy(lasso_pred_train, train_data$Winner)-accuracy(lasso_pred_test, test_data$Winner))
)



# Create a pander table
pander(comparison_df)
```

The characteristics inherent to the problem of a binary classification of volleyball matches pose a challenge in defining a singular, ideal metric for evaluating model performance. Since our dataset presents a balanced predicted variable, as each match appears only once, taking into account both home and guest team perspectives, the classification threshold is arbitrarily set at 0.5. Based on that information, one may consider accuracy as a viable metric as it reflects the proportion of correctly classified matches. However, the limitation of the accuracy metric lies in its binary nature: predictions are exclusively deemed correct or incorrect, with no consideration given to the magnitude of the prediction error, an aspect which could bear significant relevance in a betting context.
An alternate measure worth considering is the Expected Calibration Error (ECE) used by Guo et al. (2017). ECE groups observations into probability bins and calculates the absolute difference between the mean predicted probability within the bin and the empirical probability of said bin. This difference is then weighted by the proportion of all observations falling into each bin and summed into single number. The primary advantage of ECE lies in its holistic approach as it factors in not solely the correctness of a prediction, but also the extent to which different probability levels align with empirical ratios.
Overfitting was a recurring issue during the training of tree-based models. The difference between test and training sample accuracy served as an indicator of this phenomenon and was rigorously examined.
Throughout the model training and selection processes, all three aforementioned metrics were taken into consideration. The primary aim during hyperparameter tuning was to maintain the accuracy discrepancy between the training and test sets within 3% threshold, followed by maximising accuracy and minimising ECE. As these metrics are inherently incomparable, the decision-making process was arbitrary.

Finding the perfect measure for the volleyball match prediction model isn't straightforward due to the nature of binary classification problems. As each match shows up only once from both teams' perspectives, the data is fairly balanced, soI have set the classification threshold at the level of 0.5. One could argue accuracy, the rate of correctly predicted matches, is a suitable measure. However, accuracy's flaw is that it's binary: it only counts predictions as right or wrong without considering how off the error was, which could be crucial in betting scenarios.
Another potentially useful metric is the Expected Calibration Error (ECE) by Guo et al. (2017). ECE bins observations based on their predicted probabilities, calculates the absolute difference between the mean predicted and actual probability in each bin, weights these differences by the proportion of observations in each bin, and adds them up. ECE's strength lies in its comprehensive approach, as it looks at not just the accuracy, but also how well various probability levels match real-world rates. During the training of tree-based models, overfitting was a common problem. This problem was therefore monitored by including the gap between test and training accuracy into models comparison.

All of the mentioned metrics were considered while training and selecting models. During hyperparameter tuning, my main goal was to keep the accuracy gap between training and test sets under a 3% threshold, maximize accuracy, and minimize ECE. Since these metrics can't really be compared, I had to make the arbitrary decisions. The comparison shown in the table above, reveals a similar performance across all the techniques. Given that the difference in accuracy between the train and test samples is comparable for all models except the Random Forest, I'm disregarding this model in my further analysis and focusing on other metrics. When it comes to the Expected Calibration Error, the LOGIT, SVM, and Lasso models perform similarly, with errors hovering around the 2% mark. For accuracy, the XGB model stands out, achieving 0.829 on the test sample, closely followed by the Lasso model at 0.816. Since my goal was to both maximize accuracy and minimize the ECE metric, I'll be considering both the XGB and Lasso models in my ongoing analysis. 

Underneath one could see the model fit to various probability buckets, which is source to Expected Calibration Error calculation for the two considered models. Both the eXtreme Gradient Boosting and Lasso models perform well on the Calibration Plots.The first model shows a consistent trend of overestimating the winning probability for team 1 until around 0.7, after which it starts to underestimate it. This model seems to favor the underdog in its predictions. On the other hand, the Lasso algorithm handles extreme cases well, but it tends to underestimate predictions for mid-range probabilities, favoring the home team's chances. While this could be an issue, only 12.5% of records fall into the four categories where the difference between the predicted and actual probabilities is over 5 percentage points. This could explain why the model doesn't focus too much on this part of the distribution.

```{r, include=FALSE, warning=FALSE}

# Validations -------------------------------------------------------------

validation_graph <- test_data[,c(1,2,4,7)]
# count duplicates in test_data


validation_graph$logit_pred <- logit_test_fitted
validation_graph$rf_pred <- pred.test.randomforest4
validation_graph$xgb_pred <- pred.test.xgb2
validation_graph$svm_pred <- SVM_linear_fitted_prob_test[,1]
validation_graph$lasso_pred <- lasso_pred_test[,"s1"]


# create the bucket column

real_bucket_middle_points <- function(input, n_buckets) {
  bucket_width = 1 / n_buckets
  buckets <- cut(input, seq(0, 1, bucket_width), include.lowest = TRUE)
  
  # Create a data frame with your input and its corresponding bucket
  data <- data.frame(input = input, bucket = buckets)
  
  # Group by bucket and calculate the mean
  bucket_means <- aggregate(input ~ bucket, data, mean)
  
  # Create a named vector to use for matching
  bucket_mean_vec <- setNames(bucket_means$input, bucket_means$bucket)
  
  # Match the bucket mean to each data point's bucket
  mean_input <- bucket_mean_vec[as.character(buckets)]
  
  return(mean_input)
}


validation_graph$logit_bucket_middle_point <- real_bucket_middle_points(validation_graph$logit_pred, 20)
validation_graph$rf_bucket_middle_point <- real_bucket_middle_points(validation_graph$rf_pred, 20)
validation_graph$xgb_bucket_middle_point <- real_bucket_middle_points(validation_graph$xgb_pred, 20)
validation_graph$svm_bucket_middle_point <- real_bucket_middle_points(validation_graph$svm_pred, 20)
validation_graph$lasso_bucket_middle_point <- real_bucket_middle_points(validation_graph$lasso_pred, 20)



# Probability buckets vs actual wins --------------------------------------

logit_calibration <- validation_graph %>%
  group_by(logit_bucket_middle_point) %>%
  summarize(true_logit_percentage = mean(Winner == "TRUE.")) %>%
  ggplot(aes(x = logit_bucket_middle_point, y = true_logit_percentage)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Expected probability", y = "Fraction of positives",
       title = "LOGIT model predictions vs truly won") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

rf_calibration <- validation_graph %>%
  group_by(rf_bucket_middle_point) %>%
  summarize(true_rf_percentage = mean(Winner == "TRUE.")) %>%
  ggplot(aes(x = rf_bucket_middle_point, y = true_rf_percentage)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Expected probability", y = "Fraction of positives",
       title = "Random Forest model predictions vs truly won") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

xgb_calibration <- validation_graph %>%
  group_by(xgb_bucket_middle_point) %>%
  summarize(true_xgb_percentage = mean(Winner == "TRUE.")) %>%
  ggplot(aes(x = xgb_bucket_middle_point, y = true_xgb_percentage)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "purple") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Expected probability", y = "Fraction of positives")+
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

svm_calibration <- validation_graph %>%
  group_by(svm_bucket_middle_point) %>%
  summarize(true_svm_percentage = mean(Winner == "TRUE.") ) %>%
  ggplot(aes(x = svm_bucket_middle_point, y = true_svm_percentage)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "magenta") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Expected probability", y = "Fraction of positives",
       title = "SVM model predictions vs truly won") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

lasso_calibration <- validation_graph %>%
  group_by(lasso_bucket_middle_point) %>%
  summarize(true_lasso_percentage = mean(Winner == "TRUE.")) %>%
  ggplot(aes(x = lasso_bucket_middle_point, y = true_lasso_percentage)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "dark green") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Expected probability", y = "Fraction of positives") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")
```
```{r, fig.width = 10, fig.height = 5, echo=FALSE, warning=FALSE}

# arrange in a grid
grid.arrange(grobs = list(xgb_calibration, lasso_calibration), ncol = 2) 

```

## Hypotheses testing

```{r, include=FALSE, warning=FALSE}
validation_graph %>%
  group_by(lasso_bucket_middle_point) %>%
  summarize(true_logit_percentage = mean(Winner == "TRUE.") * 100,
            weight = n())


# Create match time buckets
validation_graph <- validation_graph %>% group_by(MatchID) %>% mutate(row_count = row_number())

validation_graph <- validation_graph %>%
  group_by(MatchID) %>%
  mutate(row_count = row_number(),
         max_count = max(row_count),
         percent_match = row_count/max_count) %>%
  ungroup()

validation_graph %>%
  filter(!(max_count %in% c(39, 43, 96))) %>%
  summarise(min_max_count = min(max_count),
            max_max_count = max(max_count))

  validation_graph[validation_graph$max_count == 117,1]

validation_graph$time_bucket <- real_bucket_middle_points(validation_graph$percent_match, 100)
# Accuracy over relative time ---------------------------------------------


# time bucket charts
logit_relative_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(logit_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(time_bucket) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  ggplot(aes(x = time_bucket, y = accuracy)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "blue")  +
  geom_smooth(color = "blue", linewidth = 0.1)+
  labs(x = "Match progress bucket (%)", y = "Predictions accuracy", 
       title = "LOGIT predictions accuracy over relative time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

rf_relative_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(rf_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(time_bucket) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  ggplot(aes(x = time_bucket, y = accuracy)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "red") +
  geom_smooth(color = "red", linewidth = 0.1)+
  labs(x = "Match progress bucket (%)", y = "Predictions accuracy", 
       title = "Random Forest predictions accuracy over relative time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

xgb_relative_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(xgb_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(time_bucket) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  ggplot(aes(x = time_bucket, y = accuracy)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "purple") +
  geom_smooth(color = "purple", linewidth = 0.1)+
  labs(x = "Match progress bucket (%)", y = "Predictions accuracy", 
       title = "XGBoost predictions accuracy over relative time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")


svm_relative_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(svm_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(time_bucket) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  ggplot(aes(x = time_bucket, y = accuracy)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "magenta") +
  geom_smooth(color = "magenta", linewidth = 0.1)+
  labs(x = "Match progress bucket (%)", y = "Predictions accuracy", 
       title = "SVM predictions accuracy over relative time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

lasso_relative_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(lasso_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(time_bucket) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  ggplot(aes(x = time_bucket, y = accuracy)) +
  geom_point(size = 3, shape = 21, fill = "white", color = "dark green") +
  geom_smooth(color = "dark green", linewidth = 0.1)+
  labs(x = "Match progress bucket (%)", y = "Predictions accuracy", 
       title = "Lasso predictions accuracy over relative time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")
```
```{r, fig.width = 10, fig.height = 10, echo=FALSE, warning=FALSE}
grid.arrange(grobs = list(logit_relative_time, xgb_relative_time, svm_relative_time, lasso_relative_time), ncol = 2)
```

Charts above show how accuracy changes over relative time intervals for the chosen models, testing the hypothesis about information gain throughout the game. I assigned each observation a point number, counting from the start of each game, and normalized it by dividing the obtained number by the max point number for that game. This gave me a column showing the percentage progress of the match. I then grouped these values into 100 buckets, and calculated the accuracy for each bucket, adding trend lines for visualization clarity.

All models show a similar trend of increasing accuracy over time, though the growth slows down around the match's midpoint. Looking at each bucket of the Logit, SVM, and Lasso models separately, there's a small drop in accuracy in the middle period. However, a smoothing line shows a steady upward trend in accuracy. The eXtreme Gradient Boosting model stands out with its slower growth trajectory, and only slight changes around the midpoint. This anomaly around the match's middle suggests it's a crucial period where the game's outcome often changes. Even though there aren't theoretical explanations for these shifts, the predicted odds suggest the middle phase is a time of heightened uncertainty. The eXtreme Gradient Boosting model's smoother increase in accuracy implies it might rely more on long-term situation variables, while other models seem more affected by short-term changes. A visual comparison of the models' smoothing lines shows the XGB model outdoing the others in prediction accuracy for about the first 80% of the match progress. Even though there's a slowdown in accuracy growth at a certain point in each model, instances of decreased accuracy are negligible. Based on these graphs, it's clear that prediction accuracy does indeed improve as the game goes on.


```{r, include = FALSE, warning=FALSE}
set_mean_rows <- data.frame()

for (i in 1:5) {
  temp_result <- validation_graph %>%
    group_by(MatchID) %>%
    filter(Set_number == i) %>%
    summarise(max_row_count = max(row_count)) %>%
    summarise(mean_max_row_count = mean(max_row_count))
  
  temp_result$set_number <- i
  set_mean_rows <- rbind(set_mean_rows, temp_result)
}
set_mean_rows$prev_mean_max_row_count <- lag(set_mean_rows$mean_max_row_count)
set_mean_rows$label_position <- (set_mean_rows$mean_max_row_count + set_mean_rows$prev_mean_max_row_count) / 2
set_mean_rows$label_position[1] <- set_mean_rows$mean_max_row_count[1] / 2


logit_acc_over_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(logit_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(row_count) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  mutate(cutoff = quantile(row_count, 0.9)) %>%
  filter(row_count <= cutoff) %>%
  ggplot(aes(x = row_count, y = accuracy)) +
  geom_point(size = 2, shape = 21, fill = "white", color = "black") +
  geom_smooth(color = "blue") +
  ylim(0.6, 1) +
  labs(x = "Point number", y = "Predictions accuracy", 
       title = "LOGIT predictions accuracy over time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")



rf_acc_over_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(rf_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(row_count) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  mutate(cutoff = quantile(row_count, 0.9)) %>%
  filter(row_count <= cutoff) %>%
  ggplot(aes(x = row_count, y = accuracy)) +
  geom_point(size = 2, shape = 21, fill = "white", color = "black") +
  geom_smooth(color = "red") +
  ylim(0.6, 1) +
  labs(x = "Point number", y = "Predictions accuracy", 
       title = "Random Forest predictions accuracy over time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

xgb_acc_over_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(xgb_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(row_count) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  mutate(cutoff = quantile(row_count, 0.9)) %>%
  filter(row_count <= cutoff) %>%
  ggplot(aes(x = row_count, y = accuracy)) +
  geom_point(size = 2, shape = 21, fill = "white", color = "black") +
  geom_smooth(color = "purple") +
  ylim(0.6, 1) +
  labs(x = "Point number", y = "Predictions accuracy", 
       title = "XGBoost predictions accuracy over time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

svm_acc_over_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(svm_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(row_count) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  mutate(cutoff = quantile(row_count, 0.9)) %>%
  filter(row_count <= cutoff) %>%
  ggplot(aes(x = row_count, y = accuracy)) +
  geom_point(size = 2, shape = 21, fill = "white", color = "black") +
  geom_smooth(color = "magenta") +
  ylim(0.6, 1) +
  labs(x = "Point number", y = "Predictions accuracy", 
       title = "SVM predictions accuracy over time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

lasso_acc_over_time <- validation_graph %>%
  mutate(prediction = factor(ifelse(lasso_pred > 0.5, # condition
                                    "TRUE.", # what returned if condition TRUE
                                    "FALSE.")),
         prediction_success = ifelse(prediction == Winner, 1,0)) %>%
  group_by(row_count) %>%
  summarize(accuracy = sum(prediction_success) / n()) %>%
  mutate(cutoff = quantile(row_count, 0.9)) %>%
  filter(row_count <= cutoff) %>%
  ggplot(aes(x = row_count, y = accuracy)) +
  geom_point(size = 2, shape = 21, fill = "white", color = "black") +
  geom_smooth(color = "dark green") +
  ylim(0.6, 1) +
  labs(x = "Point number", y = "Predictions accuracy", 
       title = "Lasso predictions accuracy over time") +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14, color="black"),
        axis.title.x = element_text(face="bold", size=12, color="black"),
        axis.title.y = element_text(face="bold", size=12, color="black"),
        axis.text = element_text(size=12),
        legend.position = "none")

add_lines_and_labels <- function(plot) {
  for (i in 1:nrow(set_mean_rows)) {
    plot <- plot + 
      geom_vline(xintercept = set_mean_rows$mean_max_row_count[i], linetype = "dashed", color = "blue") +
      annotate("text", x = set_mean_rows$label_position[i], y = Inf, label = paste("Set", set_mean_rows$set_number[i]), vjust = 1.05, color = "black")
  }
  return(plot)
}

logit_acc_over_time <- add_lines_and_labels(logit_acc_over_time)
rf_acc_over_time <- add_lines_and_labels(rf_acc_over_time)
xgb_acc_over_time <- add_lines_and_labels(xgb_acc_over_time)
svm_acc_over_time <- add_lines_and_labels(svm_acc_over_time)
lasso_acc_over_time <- add_lines_and_labels(lasso_acc_over_time)



# list of plots
plot_list <- list(logit_acc_over_time, xgb_acc_over_time, svm_acc_over_time, lasso_acc_over_time)

```
```{r, fig.width = 10, fig.height = 10, echo=FALSE, warning=FALSE}

# arrange in a grid
grid.arrange(grobs = plot_list, ncol = 2)  # 2 column layout
```

Set of charts above show how four models perform throughout absolute time, tracking their accuracy throughout a match, measured in point numbers. I've added trend lines to the scatterplot for easier analysis, and five vertical lines to mark the average end point of each set. The LOGIT, SVM, and Lasso models follow a similar hyperbolic pattern, peaking between the second and third set before declining and then rebounding near the end of the fourth set. The XGB model, on the other hand, shows a linear growth from the start to the middle of the third set, followed by a steady decline. A comparison of the models' smoothing lines reveals a shift in the critical point between the Logit, SVM, and Lasso models and the XGBoost model. The XGBoost model reaches this point later, achieving higher accuracy than the Lasso model at its peak. Despite the timing differences, the graph demonstrates a key transition in all models where prediction accuracy starts to fall as the match progresses.

The turning point in prediction accuracy may be due to the varying length of volleyball matches. Only 394 of the 697 matches in the dataset went beyond 3 games, and just 157 lasted for 5 sets. This could inflate the accuracy in the third set area of the graphs, due to games nearing their end with a clear winner. As the total points progress, matches that have ended are excluded, so after the 135th point, the average end of the third set, only 73.2% of the test dataset was included. After the 182nd point, the average end of the fourth set, only 36.8% of the observations were included, mostly matches tied at two sets each with no clear winner. I believe the turning point in prediction accuracy happens because from the third set on, the number of considered games decreases, and the share of tied games, expected to end later, increases.
This suggests that there's a point in the match when bookmakers could stop accepting in-game bets as the odds predicted with high accuracy decline. Identifying this point could help bookmakers reduce potential losses by not accepting further bets once predictability drops. This gives bookmakers a new way to manage in-game bets effectively and align their operations with their best interests.

## Conclusions

The point-by-point prediction approach has proven its effectiveness across various sports, both individual and team-based. It's reasonable to expect the same level of efficacy when applied to volleyball matches, an assumption I've examined in depth in this research. I considered information from 697 matches spanning from 2020-2023, obtained from the Plusliga website. After conducting an exploratory data analysis and transforming the data, the resulting dataset was divided into training and testing samples. These were then used to train and compare five machine learning algorithms. From the data, I scrutinized two hypotheses:

1. Prediction accuracy continuously improves with the relative progress of the game.
2. There's a specific point in the match after which the accuracy of predictions tends to decrease.

Through comprehensive graphical analyses, utilizing trend lines, both hypotheses were substantiated. These analyses demonstrated the constant growth of prediction accuracy in terms of relative match time and the existence of an inflection point in prediction accuracy for absolute time.
Possible extensions of this analysis could include broadening the dataset to encompass individual player statistics, including basic information about them, their match and seasonal form, and the current squad for each point. This approach could improve prediction accuracy by accounting for exceptional individual performances. Other valuable variables could include significant player injuries, tactical substitutions during matches, the history of previous encounters between teams, and the distance between the teams' home cities.In the context of model selection, I could improve the analysis by examining correlations between neighboring points, an aspect overlooked in this study as each observation was treated independently. Exploring potential point series or relationships between individual points could also yield valuable insights. The changing dynamics of match situations could be captured by applying Neural Network architectures like Long-Short Term Memory models.
This study explores an innovative approach to predicting in-game odds for volleyball match outcomes, using the Polish men's professional volleyball league, Plusliga, as a case study. Given that research in this field is less developed compared to sports analytics research for more popular sports like American Football, Basketball, and Tennis, there's significant potential for further work. As demonstrated in this research, machine learning techniques are well-suited for analyzing volleyball matches.


## Bibliography

* Breiman, L. (2001). Random forests. Machine learning, 45, 5-32.
* Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794).
* Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine learning, 20, 273-297.
* Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In International conference on machine learning (pp. 1321-1330). PMLR.
* Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.